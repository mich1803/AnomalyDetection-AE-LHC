{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mich1803/AnomalyDetection-AE-LHC/blob/main/AD_AE_LHC_PhysAI2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ix-CMEfVfV2"
      },
      "source": [
        "# Group Project 2024/25\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs3lL8NouR0R"
      },
      "source": [
        "## Task\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtrtaHDwuV_R"
      },
      "source": [
        "\n",
        "Allenare un AE per Anomaly Detection in grado di riconoscere segnali prodotti da jeti adronici anomali in un reivelatore di fisica delle alte energie.\n",
        "\n",
        "Alle energie estreme del Large Hadron Collider, particelle massive possono essere prodotte con un tale boost di Lorentz da far sì che i loro decadimenti in adroni (getti adronici) risultino così collimati che le particelle prodotte si sovrappongono. Determinare se la sottostruttura di un getto osservato sia dovuta a una singola particella di bassa massa oppure a molteplici prodotti di decadimento di una particella di massa elevata è un problema cruciale nell’analisi dei dati del LHC. Gli approcci tradizionali si basano su osservabili di alto livello costruite a partire da modelli teorici di deposizione di energia nei calorimetri e da parametri delle tracce cariche ricostruite nel tracciatore interno, ma la complessità dei dati rende questo compito un candidato ideale per l’applicazione di strumenti di deep learning. I costituenti dei getti possono infatti essere rappresentati come immagini 2D in cui ogni pixel rappresenta una delle celle sensibili del calorimetro, e il contenuto della cella una misura dell'energia o della quantità di moto depositata nella cella.\n",
        "\n",
        "**Dataset:**\n",
        "\n",
        "I dati del progetto sono nella forma di immagini 2D di dimensione (100,100), ogni cella rappresenta l'energia depositata in quella cella dalle particelle del jet adronico corrispondente. Ci sono due tipologie di jet adronici consider ati: *jet normali*, costituiti dalla adronizzazione di un quark leggero o gluone, e *jet anomali* (disponibili in una frazione incognita solo nel test set) costituiti dall'adronizzazione dei quark nel decadimento $t \\to Wb \\to qq'b$, in cui a causa del boost del quark top, i tre quark nello stato finale sono parzialmente sovrapposti.\n",
        "\n",
        "* *Normal data dataset:* 12k jet rappresentati come histogrammi 2D della quantità di moto depositata in ciascuno dei 100x100 bin di una finestra quadrata nel piano ($\\theta,\\phi$) centrato intorno all'asse del jet.\n",
        "\n",
        "* *Test dataset:*\n",
        "due dataset costituiti ciascuno da 3k eventi, contenenti jet normali e jet anomali in una frazione relativa icognita da determinare. Nel primo dataset (*_high*) la frazione incognita di eventi anomali è $\\ge 55\\%$. Nel secondo dataset (*_low*) la frazione incognita di eventi anomali incognita è $\\le 45\\%$.\n",
        "Potete utilizzare questa informazione per verificare che le vostre predizioni soddisfino la relazione $f_{high} > f_{low}$.\n",
        "\n",
        "I dati sono forniti come array numpy in un file numpy compresso (.npz), leggibile con l'esempio di codice che segue:\n",
        "\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "\n",
        "f_train = np.load('Normal_data.npz')\n",
        "f_test_l = np.load('Test_data_low.npz')\n",
        "f_test_h = np.load('Test_data_high.npz')\n",
        "\n",
        "normal_data = f_train['normal_data']\n",
        "test_data_l = f_test_l['test_data']\n",
        "test_data_h = f_test_h['test_data']\n",
        "\n",
        "print(normal_data.shape)\n",
        "print(test_data_l.shape)\n",
        "print(test_data_h.shape)\n",
        "```\n",
        "\n",
        "**Per scaricare i dataset:**\n",
        "* dati normali:\n",
        "```\n",
        "!wget http://giagu.web.cern.ch/giagu/CERN/P2025/Normal_data.npz\n",
        "```\n",
        "* dati anomali:\n",
        "```\n",
        "!wget http://giagu.web.cern.ch/giagu/CERN/P2025/<Identificativo Dataset>/Test_data_low.npz\n",
        "!wget http://giagu.web.cern.ch/giagu/CERN/P2025/<Identificativo Dataset>/Test_data_high.npz\n",
        "```\n",
        "```\n",
        "# <Identificativo Dataset> dal foglio excel prenotazione gruppi\n",
        "```\n",
        "\n",
        "\n",
        "**Obiettivi minimi del progetto (potete a vostro piacimento aggiungere ulteriori analisi/studi:**\n",
        "\n",
        "1. Plot della rappresentazione latente delle immagini di test fatto con riduzione dimensionale.\n",
        "2. Stima della frazione di eventi anomali presente nei due Test dataset, tenendo conto che la di procedura di stima deve garantire che la rate di falsi postivi sia inferiore a circa il $10\\%$ (FPR $\\le \\sim 10\\%$).\n",
        "3. Clustering dello spazio (per esempio usando un algoritmo GMM).\n",
        "4. Misura della purezza dei cluster rispetto alle label assegnate in anomaly score.\n",
        "\n",
        "\n",
        "**Nota Importante:**\n",
        "\n",
        "Il notebook deve essere compilato come una relazione scientifica del progetto, quindi deve contenere sia il codice (leggibile e riproducibile), i risultati in termini di grafici e tabelle numeriche, e il testo che illustra la strategia ottenuta, le scelte compiute, e i risultati ottenuti."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHD01QBg2Z3Y"
      },
      "source": [
        "## Introduzione"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfgJcItSAKnI"
      },
      "source": [
        "In questo progetto proponiamo un approccio basato su Autoencoder convolutivi per l'identificazione di anomalie nei segnali prodotti da jet adronici in un rivelatore del Large Hadron Collider. Le immagini utilizzate rappresentano mappe bidimensionali della quantità di moto depositata nei calorimetri, e vengono trattate come input per reti neurali convolutive.\n",
        "\n",
        "L'obiettivo principale è confrontare due strategie per la rilevazione di anomalie:\n",
        "1. Una basata sull'errore di ricostruzione (Mean Squared Error, MSE) tra immagine originale e immagine ricostruita;\n",
        "2. Una basata su un'analisi di clustering dello spazio latente (ottenuto tramite codifica dell'Autoencoder), in particolare utilizzando un algoritmo Gaussian Mixture Model (GMM).\n",
        "\n",
        "Per svolgere tale confronto sono stati addestrati due modelli di Autoencoder con architettura convolutiva:\n",
        "\n",
        "- **Modello S**: spazio latente a 3 dimensioni, direttamente visualizzabile in 3D.\n",
        "- **Modello L**: spazio latente a 32 dimensioni, successivamente ridotto a 3D tramite PCA per scopi di visualizzazione e clustering.\n",
        "\n",
        "Per entrambi i modelli, l'addestramento è stato effettuato unicamente su eventi normali, mentre il test è stato condotto su due dataset contenenti una frazione ignota di eventi anomali.\n",
        "\n",
        "**Autori**  \n",
        "Michele Magrini (2066963)  \n",
        "Julian Hendrix (2090880)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwlpgVGjuarB"
      },
      "source": [
        "## Codice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQimlXyGXiJ0"
      },
      "source": [
        "**Framework utilizzati**\n",
        "\n",
        "Per la fase di training del modello Autoencoder convolutivo abbiamo scelto di utilizzare il framework **PyTorch Lightning**, che consente una gestione modulare e pulita del codice, semplificando l'implementazione del training loop, la gestione del dispositivo (CPU/GPU) e l'integrazione con callback come l'early stopping.\n",
        "\n",
        "Per la riduzione dimensionale e il clustering nello spazio latente abbiamo invece utilizzato **scikit-learn**, in particolare:\n",
        "- **PCA (Principal Component Analysis)** per proiettare in 3D gli spazi latenti di dimensione superiore (es. 32D o 64D) e facilitarne la visualizzazione.\n",
        "- **Gaussian Mixture Model (GMM)** per effettuare il clustering delle rappresentazioni latenti, utile a stimare la separazione tra eventi normali e anomali in modo non supervisionato.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKykx92QJO18"
      },
      "outputs": [],
      "source": [
        "#@title Import librerie, settaggio seed per la riproducibilità\n",
        "!pip install -q pytorch-lightning\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn.functional as F\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import plotly.graph_objs as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Impostazioni per la riproducibilità\n",
        "SEED = 42\n",
        "pl.seed_everything(SEED, workers=True)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "\n",
        "# Device (CPU/GPU)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {DEVICE} device\")\n",
        "\n",
        "# Download dei dataset\n",
        "ID = \"G15\"\n",
        "!wget -nc http://giagu.web.cern.ch/giagu/CERN/P2025/Normal_data.npz\n",
        "!wget -nc http://giagu.web.cern.ch/giagu/CERN/P2025/{ID}/Test_data_low.npz\n",
        "!wget -nc http://giagu.web.cern.ch/giagu/CERN/P2025/{ID}/Test_data_high.npz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKQgMI5O0WZ7"
      },
      "source": [
        "### Caricamento dei Dataset/Dataloader e visualizzazione preliminare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_heiiYxsX-mF"
      },
      "source": [
        "**Suddivisione del dataset**\n",
        "\n",
        "Abbiamo deciso di suddividere il dataset di training, contenente esclusivamente eventi normali, in due parti:\n",
        "- **Training set**: 90% dei dati, utilizzato per l'addestramento dell'Autoencoder.\n",
        "- **Validation set**: 10% dei dati, usato per due scopi principali:\n",
        "  - Implementazione dell'**early stopping**, in modo da evitare overfitting interrompendo il training quando le prestazioni sul validation set non migliorano più.\n",
        "  - Definizione di una **soglia sull'errore di ricostruzione (MSE)** al 90° percentile, utilizzata come criterio per classificare un evento come anomalo durante l'inferenza.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsbJ-Pb_a_og"
      },
      "outputs": [],
      "source": [
        "#@title Creazione Datasets e DataLoader PyTorch\n",
        "\n",
        "f_train = np.load('Normal_data.npz')\n",
        "f_test_l = np.load('Test_data_low.npz')\n",
        "f_test_h = np.load('Test_data_high.npz')\n",
        "\n",
        "normal_data = f_train['normal_data']\n",
        "test_data_l = f_test_l['test_data']\n",
        "test_data_h = f_test_h['test_data']\n",
        "\n",
        "# Normalizzazione [0,1] usando solo il training set\n",
        "train_min = np.min(normal_data)\n",
        "train_max = np.max(normal_data)\n",
        "\n",
        "def normalize(data, data_min, data_max):\n",
        "    return (data - data_min) / (data_max - data_min + 1e-8)\n",
        "\n",
        "normal_data = normalize(normal_data, train_min, train_max)\n",
        "test_data_l = normalize(test_data_l, train_min, train_max)\n",
        "test_data_h = normalize(test_data_h, train_min, train_max)\n",
        "\n",
        "# Conversione in tensori torch\n",
        "normal_tensor = torch.tensor(normal_data, dtype=torch.float32).unsqueeze(1)\n",
        "test_tensor_l = torch.tensor(test_data_l, dtype=torch.float32).unsqueeze(1)\n",
        "test_tensor_h = torch.tensor(test_data_h, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Split: 90% train, 10% val\n",
        "train_len = int(0.9 * len(normal_tensor))\n",
        "val_len = len(normal_tensor) - train_len\n",
        "full_dataset = TensorDataset(normal_tensor)\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_len, val_len])\n",
        "\n",
        "# Dataset\n",
        "test_dataset_l = TensorDataset(test_tensor_l)\n",
        "test_dataset_h = TensorDataset(test_tensor_h)\n",
        "\n",
        "# DataLoader\n",
        "BATCH_SIZE = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader_l = DataLoader(test_dataset_l, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader_h = DataLoader(test_dataset_h, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Info\n",
        "print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test Low: {len(test_dataset_l)} | Test High: {len(test_dataset_h)}\")\n",
        "# Print Sample Shape\n",
        "print(f\"Train Sample Shape: {train_dataset[0][0].shape}\")\n",
        "\n",
        "datasets = {\n",
        "    \"Train (Normal)\": normal_data,\n",
        "    \"Test Low\": test_data_l,\n",
        "    \"Test High\": test_data_h\n",
        "}\n",
        "print(\"\\nLa normalizzazione tra 0 e 1 è basata sui dati di training:\")\n",
        "for name, data in datasets.items():\n",
        "    print(f\"{name}: min = {np.min(data):.4f}, max = {np.max(data):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADkE7_elwPBQ"
      },
      "outputs": [],
      "source": [
        "#@title plot di alcuni eventi del dataset\n",
        "\n",
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(10, 10))\n",
        "fig.suptitle(\"Primi 3 esempi per dataset (100x100)\\nScala dei colori locale per ciascuna immagine\", fontsize=14)\n",
        "\n",
        "for row_idx, (label, data) in enumerate(datasets.items()):\n",
        "    for col_idx in range(3):\n",
        "        ax = axes[row_idx, col_idx]\n",
        "        img = data[col_idx]\n",
        "        im = ax.imshow(img, cmap=\"inferno\", origin=\"lower\", vmin=np.min(img), vmax=np.max(img))\n",
        "        ax.axis(\"off\")\n",
        "        if col_idx == 0:\n",
        "            ax.set_title(label, fontsize=12, loc='left')\n",
        "\n",
        "fig.subplots_adjust(left=0.05, right=0.95, top=0.9, bottom=0.05, wspace=0.1, hspace=0.2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47ZfbzL884h-"
      },
      "source": [
        "### Definizione del Modello e dei Parametri\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H482v963YzH7"
      },
      "source": [
        "**Architettura dell'Autoencoder**\n",
        "\n",
        "L'Autoencoder implementato è basato su una struttura convolutiva simmetrica, composta da un encoder e un decoder. L'input è costituito da immagini di dimensione 100×100 con un solo canale.\n",
        "\n",
        "L'**encoder** è costituito da tre strati convolutivi con kernel 4×4, stride 2 e padding 1. Ad ogni convoluzione segue una funzione di attivazione ReLU e un livello di dropout. La sequenza di convoluzioni riduce progressivamente la risoluzione da (100×100) a (12×12) comprimendo l'informazione in un vettore di dimensione piatta pari a 9216. Questo vettore viene poi proiettato linearmente nello **spazio latente**, che può essere di dimensione 3 (Modello S) o 32 (Modello L).\n",
        "\n",
        "Il **decoder** segue una struttura speculare: un layer lineare riporta il vettore latente alla dimensione 9216, che viene poi riconvertita alla dimensione convoluzionale originale tramite tre strati di **transposed convolution**, fino a ricostruire un'immagine di output 100×100. L'ultima attivazione è una funzione **sigmoide**, coerente con la normalizzazione dei dati tra 0 e 1.\n",
        "\n",
        "La figura seguente rappresenta visivamente il flusso dati attraverso i vari livelli del modello:\n",
        "\n",
        "![AE Architecture](https://github.com/mich1803/AnomalyDetection-AE-LHC/blob/main/media/AE.png?raw=true)\n",
        "\n",
        "L'addestramento è stato effettuato tramite PyTorch Lightning con ottimizzazione Adam, early stopping sulla `val_loss`, e una suddivisione del dataset di addestramento in 90% training e 10% validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIPwVTGq89-4"
      },
      "outputs": [],
      "source": [
        "#@title Modello ConvAE\n",
        "\n",
        "class ConvAE(pl.LightningModule):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(config)\n",
        "        self.lr = config.get(\"lr\", 1e-3)\n",
        "        self.dropout = config.get(\"dropout\", 0.0)\n",
        "        hidden_dim = config[\"hidden_dim\"]\n",
        "\n",
        "        # Encoder\n",
        "        in_channels = 1\n",
        "        encoder_layers = []\n",
        "        for conv in config[\"in_conv\"]:\n",
        "            encoder_layers.append(nn.Conv2d(in_channels, **{k: v for k, v in conv.items() if k != \"output_padding\"}))\n",
        "            encoder_layers.append(nn.ReLU())\n",
        "            if self.dropout > 0:\n",
        "                encoder_layers.append(nn.Dropout2d(self.dropout))\n",
        "            in_channels = conv[\"out_channels\"]\n",
        "        self.encoder_conv = nn.Sequential(*encoder_layers)\n",
        "\n",
        "        # Calcola shape del bottleneck convoluzionale\n",
        "        dummy_input = torch.zeros(1, 1, 100, 100)\n",
        "        with torch.no_grad():\n",
        "            dummy_out = self.encoder_conv(dummy_input)\n",
        "            self.conv_shape = dummy_out.shape[1:]  # (C, H, W)\n",
        "            self.flat_dim = dummy_out.numel()\n",
        "\n",
        "\n",
        "        # Bottleneck FC\n",
        "        self.encoder_fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(self.flat_dim, hidden_dim)\n",
        "        )\n",
        "        self.decoder_fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, self.flat_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        decoder_layers = []\n",
        "        in_channels = config[\"in_conv\"][-1][\"out_channels\"]\n",
        "        for conv in config[\"out_conv\"]:\n",
        "            decoder_layers.append(nn.ConvTranspose2d(in_channels, **conv))\n",
        "            decoder_layers.append(nn.ReLU())\n",
        "            if self.dropout > 0:\n",
        "                decoder_layers.append(nn.Dropout2d(self.dropout))\n",
        "            in_channels = conv[\"out_channels\"]\n",
        "        decoder_layers[-2] = nn.Sigmoid()  # ultima attivazione\n",
        "        self.decoder_conv = nn.Sequential(*decoder_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder_conv(x)\n",
        "        x = self.encoder_fc(x)\n",
        "        x = self.decoder_fc(x)\n",
        "        x = x.view(-1, *self.conv_shape)  # usa shape salvata\n",
        "        x = self.decoder_conv(x)\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x = batch[0]\n",
        "        x_hat = self(x)\n",
        "        loss = F.mse_loss(x_hat, x)\n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x = batch[0]\n",
        "        x_hat = self(x)\n",
        "        loss = F.mse_loss(x_hat, x)\n",
        "        self.log(\"val_loss\", loss, prog_bar=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxFOPJ_f-cpE"
      },
      "outputs": [],
      "source": [
        "#@title Parametri S e L\n",
        "\n",
        "# Small (3D spazio latente)\n",
        "params_S = {\n",
        "    \"hidden_dim\": 3,\n",
        "    \"in_conv\": [\n",
        "        {\"out_channels\": 16, \"kernel_size\": 4, \"stride\": 2, \"padding\": 1},  # 100 → 50\n",
        "        {\"out_channels\": 32, \"kernel_size\": 4, \"stride\": 2, \"padding\": 1},  # 50 → 25\n",
        "        {\"out_channels\": 64, \"kernel_size\": 4, \"stride\": 2, \"padding\": 1}   # 25 → 13\n",
        "    ],\n",
        "    \"out_conv\": [\n",
        "        {\"out_channels\": 32, \"kernel_size\": 4, \"stride\": 2, \"padding\": 1, \"output_padding\": 1},  # 13 → 25\n",
        "        {\"out_channels\": 16, \"kernel_size\": 4, \"stride\": 2, \"padding\": 1, \"output_padding\": 0},  # 25 → 50\n",
        "        {\"out_channels\": 1,  \"kernel_size\": 4, \"stride\": 2, \"padding\": 1, \"output_padding\": 0}   # 50 → 100\n",
        "    ],\n",
        "    \"dropout\": 0.2,\n",
        "    \"lr\": 1e-3\n",
        "}\n",
        "\n",
        "# Large (32D spazio latente)\n",
        "params_L = {**params_S, \"hidden_dim\": 32}\n",
        "\n",
        "MAX_EPOCHS = 50\n",
        "PATIENCE = MAX_EPOCHS // 5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3F04lVy_jUI"
      },
      "source": [
        "### Modello S -> 3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAcAG8oOisFI"
      },
      "outputs": [],
      "source": [
        "# Inizializza il modello\n",
        "from torchsummary import summary\n",
        "model_S = ConvAE(params_S).to(DEVICE)\n",
        "\n",
        "summary(model_S, input_size=(1, 100, 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9G4hl5f_gjm"
      },
      "outputs": [],
      "source": [
        "#@title Training del modello Small\n",
        "\n",
        "model_S = ConvAE(params_S)\n",
        "\n",
        "early_stop_cb = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=PATIENCE,\n",
        "    mode=\"min\"\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    max_epochs=MAX_EPOCHS,\n",
        "    accelerator=\"auto\",\n",
        "    callbacks=[early_stop_cb],\n",
        "    log_every_n_steps=10\n",
        ")\n",
        "\n",
        "# Training\n",
        "trainer.fit(model_S, train_loader, val_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HclMfdNfDRD_"
      },
      "outputs": [],
      "source": [
        "#@title Confronto degli errori di ricostruzione: Test LOW, Test HIGH\n",
        "\n",
        "def compute_reconstruction_errors(model, dataloader):\n",
        "    model.eval()\n",
        "    errors = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            x = batch[0].to(model.device)\n",
        "            x_hat = model(x)\n",
        "            mse = F.mse_loss(x_hat, x, reduction='none')\n",
        "            mse_per_image = mse.view(mse.size(0), -1).mean(dim=1)\n",
        "            errors.extend(mse_per_image.cpu().numpy())\n",
        "    return np.array(errors)\n",
        "\n",
        "# Calcolo errori\n",
        "train_errors = compute_reconstruction_errors(model_S, train_loader)\n",
        "val_errors = compute_reconstruction_errors(model_S, val_loader)\n",
        "test_errors_l = compute_reconstruction_errors(model_S, test_loader_l)\n",
        "test_errors_h = compute_reconstruction_errors(model_S, test_loader_h)\n",
        "\n",
        "print(f\"Train MSE: {np.mean(train_errors):.7f} | Val MSE: {np.mean(val_errors):.7f}\")\n",
        "print(f\"Test LOW MSE: {np.mean(test_errors_l):.7f} | Test HIGH MSE: {np.mean(test_errors_h):.7f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQ7rikRqNwDJ"
      },
      "outputs": [],
      "source": [
        "#@title (TASK 1) Plot 3D delle rappresentazioni latenti di: Train+Val, Test LOW, Test HIGH\n",
        "\n",
        "def extract_latents(dataloader, model):\n",
        "    latents = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            x = batch[0].to(model.device)\n",
        "            z = model.encoder_fc(model.encoder_conv(x))\n",
        "            latents.append(z.cpu().numpy())\n",
        "    return np.concatenate(latents, axis=0)\n",
        "\n",
        "# Estrazione\n",
        "latents_train = extract_latents(train_loader, model_S)\n",
        "latents_val = extract_latents(val_loader, model_S)\n",
        "latents_low = extract_latents(test_loader_l, model_S)\n",
        "latents_high = extract_latents(test_loader_h, model_S)\n",
        "\n",
        "# Funzione per creare uno scatter3D Plotly\n",
        "def plot_latents_plotly(latents, name, color):\n",
        "    return go.Scatter3d(\n",
        "        x=latents[:, 0], y=latents[:, 1], z=latents[:, 2],\n",
        "        mode='markers',\n",
        "        marker=dict(size=2, color=color, opacity=0.6),\n",
        "        name=name\n",
        "    )\n",
        "\n",
        "# Subplots Plotly\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=3,\n",
        "    specs=[[{'type': 'scatter3d'}]*3],\n",
        "    subplot_titles=[\"Train\", \"Test LOW\", \"Test HIGH\"]\n",
        ")\n",
        "\n",
        "# Tracce\n",
        "fig.add_trace(plot_latents_plotly(latents_train, \"Train\", \"deepskyblue\"), row=1, col=1)\n",
        "fig.add_trace(plot_latents_plotly(latents_val, \"Val\", \"steelblue\"), row=1, col=1)\n",
        "fig.add_trace(plot_latents_plotly(latents_low, \"Test LOW\", \"orange\"), row=1, col=2)\n",
        "fig.add_trace(plot_latents_plotly(latents_high, \"Test HIGH\", \"purple\"), row=1, col=3)\n",
        "\n",
        "# Layout\n",
        "fig.update_layout(\n",
        "    height=600, width=1800,\n",
        "    title_text=\"Confronto Spazio Latente - Train+Val vs Test\",\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GR0P6YJWYp_"
      },
      "source": [
        "**TASK 1 – Visualizzazione dello spazio latente (Modello S)**\n",
        "\n",
        "Per visualizzare lo spazio latente generato dal Modello S (con `hidden_dim = 3`), abbiamo estratto le rappresentazioni latenti delle immagini appartenenti ai dataset di training, validation, test LOW e test HIGH. Poiché la dimensione latente è tridimensionale, la rappresentazione può essere plottata direttamente in uno spazio 3D senza necessità di riduzione dimensionale.\n",
        "\n",
        "- Il primo pannello mostra lo spazio latente per i dati di training e validation.\n",
        "- Il secondo e terzo pannello mostrano rispettivamente i test LOW e HIGH.\n",
        "\n",
        "Questo tipo di visualizzazione consente un'analisi qualitativa della distribuzione dei dati nello spazio latente, utile per valutare visivamente la separabilità tra eventi normali e potenzialmente anomali. Sebbene i risultati possano variare leggermente a causa della natura stocastica del training, lo spazio latente tende a mostrare una maggiore dispersione nel test HIGH rispetto al LOW, coerente con la maggiore frazione di anomalie attese.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWrrzXJsW_N-"
      },
      "outputs": [],
      "source": [
        "#@title (TASK 2) Stima delle anomalie nei Test LOW e Test HIGH (tramite errore MSE)\n",
        "\n",
        "threshold = np.percentile(val_errors, 90)\n",
        "print(f\"Soglia impostata (90° percentile val set): {threshold:.6f}\")\n",
        "\n",
        "# Stima della frazione anomala\n",
        "frac_anom_low = np.mean(test_errors_l > threshold)\n",
        "frac_anom_high = np.mean(test_errors_h > threshold)\n",
        "\n",
        "# Plot\n",
        "fig, axs = plt.subplots(1, 3, figsize=(20, 5), sharey=True, sharex=True)\n",
        "datasets = [val_errors, test_errors_l, test_errors_h]\n",
        "titles = [\"Validation\", \"Test LOW\", \"Test HIGH\"]\n",
        "colors = ['steelblue', 'orange', 'purple']\n",
        "\n",
        "for ax, data, title, color in zip(axs, datasets, titles, colors):\n",
        "    n, bins, patches = ax.hist(data, bins=100, color=color)\n",
        "    ax.axvline(threshold, color='red', linestyle='--', label='Soglia 90° percentile')\n",
        "    for patch, left in zip(patches, bins[:-1]):\n",
        "        if left >= threshold:\n",
        "            patch.set_edgecolor('red')\n",
        "            patch.set_linewidth(2)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"MSE\")\n",
        "    ax.legend()\n",
        "\n",
        "plt.suptitle(\"Distribuzione degli errori di ricostruzione (MSE) per immagine\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFrazione stimata di anomalie:\")\n",
        "print(f\"Test LOW : {frac_anom_low:.3f}\")\n",
        "print(f\"Test HIGH: {frac_anom_high:.3f}\")\n",
        "print(f\"Verifica relazione: f_high > f_low → {frac_anom_high > frac_anom_low}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKM_r36haebW"
      },
      "source": [
        "**TASK 2 – Stima della frazione di eventi anomali tramite errore di ricostruzione**\n",
        "\n",
        "Dopo aver addestrato l'Autoencoder sul solo dataset di eventi normali, abbiamo calcolato l'errore di ricostruzione (MSE) per ciascuna immagine nei dataset di validation, test LOW e test HIGH.\n",
        "\n",
        "Abbiamo definito una soglia anomalia corrispondente al **90° percentile** degli errori di ricostruzione sul validation set. Tutti gli eventi con errore superiore a questa soglia vengono considerati anomalie.\n",
        "\n",
        "Nel grafico seguente vengono mostrati gli istogrammi della distribuzione degli errori di ricostruzione per ciascun dataset. I bin successivi alla soglia sono evidenziati con bordo rosso.\n",
        "\n",
        "I risultati ottenuti sono coerenti con le aspettative:\n",
        "- La frazione stimata di anomalie in **Test HIGH** risulta significativamente più alta rispetto a **Test LOW**.\n",
        "- La relazione $ f_{\\text{high}} > f_{\\text{low}} $ è rispettata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgyNMny1t-M2"
      },
      "outputs": [],
      "source": [
        "#@title (TASK 3) Clustering delle rappresentazioni latenti con GMM\n",
        "\n",
        "# Standardizzazione\n",
        "scaler = StandardScaler()\n",
        "latents_train_scaled = scaler.fit(latents_train)\n",
        "latents_low_scaled = scaler.transform(latents_low)\n",
        "latents_high_scaled = scaler.transform(latents_high)\n",
        "\n",
        "# Concatenazione dei dati\n",
        "latents_combined = np.concatenate([latents_low_scaled, latents_high_scaled], axis=0)\n",
        "\n",
        "# Fit del GMM su entrambi\n",
        "gmm = GaussianMixture(n_components=2, random_state=SEED)\n",
        "gmm.fit(latents_combined)\n",
        "\n",
        "# Predizione separata mantenendo coerenza dei cluster\n",
        "labels_low = gmm.predict(latents_low_scaled)\n",
        "labels_high = gmm.predict(latents_high_scaled)\n",
        "\n",
        "# Plot interattivo con subplots\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=2,\n",
        "    specs=[[{'type': 'scatter3d'}, {'type': 'scatter3d'}]],\n",
        "    subplot_titles=[\"GMM Clustering - Test LOW\", \"GMM Clustering - Test HIGH\"]\n",
        ")\n",
        "\n",
        "fig.add_trace(go.Scatter3d(\n",
        "    x=latents_low_scaled[:, 0], y=latents_low_scaled[:, 1], z=latents_low_scaled[:, 2],\n",
        "    mode='markers', marker=dict(size=2, color=labels_low, colorscale='Viridis', opacity=0.6)\n",
        "), row=1, col=1)\n",
        "\n",
        "fig.add_trace(go.Scatter3d(\n",
        "    x=latents_high_scaled[:, 0], y=latents_high_scaled[:, 1], z=latents_high_scaled[:, 2],\n",
        "    mode='markers', marker=dict(size=2, color=labels_high, colorscale='Viridis', opacity=0.6)\n",
        "), row=1, col=2)\n",
        "\n",
        "fig.update_layout(height=600, width=1000, title_text=\"GMM Clustering nello Spazio Latente\", showlegend=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi3jgkq8a-2N"
      },
      "source": [
        "**TASK 3 – Clustering dello spazio latente con GMM**\n",
        "\n",
        "Per analizzare la distribuzione degli eventi nello spazio latente appreso dal modello, abbiamo applicato un algoritmo di clustering Gaussian Mixture Model (GMM) con due componenti. Prima dell'applicazione del GMM, le rappresentazioni latenti estratte dai dataset **Test LOW** e **Test HIGH** sono state standardizzate con `StandardScaler`.\n",
        "\n",
        "Abbiamo scelto di allenare un **singolo modello GMM** concatenando i dati di entrambi i dataset (LOW e HIGH). Questo approccio garantisce che la numerazione dei cluster rimanga coerente tra i due dataset, evitando inversioni nelle etichette.\n",
        "\n",
        "Il risultato è mostrato nel seguente grafico interattivo, dove ogni punto rappresenta un jet nel suo spazio latente 3D (modello S). I colori indicano l'appartenenza al cluster GMM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5O5Mo6CxiP5"
      },
      "outputs": [],
      "source": [
        "#@title Errore di Ricostruzione VS Clustering dei Test Set\n",
        "\n",
        "fig, axs = plt.subplots(2, 2, figsize=(12, 8), sharex=True, sharey=True)\n",
        "\n",
        "# Prima riga: errori complessivi\n",
        "datasets = [test_errors_l, test_errors_h]\n",
        "titles = [\"Test LOW - Anomaly Score\", \"Test HIGH - Anomaly Score\"]\n",
        "colors = ['orange', 'purple']\n",
        "\n",
        "for ax, data, title, color in zip(axs[0], datasets, titles, colors):\n",
        "    n, bins, patches = ax.hist(data, bins=50, color=color)\n",
        "    ax.axvline(threshold, color='red', linestyle='--', label='Soglia 90° percentile')\n",
        "    for patch, left in zip(patches, bins[:-1]):\n",
        "        if left >= threshold:\n",
        "            patch.set_edgecolor('red')\n",
        "            patch.set_linewidth(2)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"MSE\")\n",
        "    ax.legend()\n",
        "\n",
        "# Seconda riga: divisione per cluster\n",
        "cluster_info = [\n",
        "    (test_errors_l, labels_low, \"Test LOW - Cluster\", ['goldenrod', 'orangered']),\n",
        "    (test_errors_h, labels_high, \"Test HIGH - Cluster\", ['indigo', 'mediumorchid'])\n",
        "]\n",
        "\n",
        "for ax, (errors, labels, title_prefix, cluster_colors) in zip(axs[1], cluster_info):\n",
        "    for cluster_id, color in zip([0, 1], cluster_colors):\n",
        "        cluster_errors = errors[labels == cluster_id]\n",
        "        ax.hist(cluster_errors, bins=50, alpha=0.7, label=f\"{title_prefix} {cluster_id}\", color=color)\n",
        "    ax.axvline(threshold, color='red', linestyle='--', label='Soglia 90° percentile')\n",
        "    ax.set_xlabel(\"MSE\")\n",
        "    ax.legend()\n",
        "    ax.set_title(f\"{title_prefix} - Anomaly Score by Cluster\")\n",
        "\n",
        "plt.suptitle(\"Distribuzione degli errori di ricostruzione (MSE) e divisione per cluster\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()\n",
        "\n",
        "print()\n",
        "\n",
        "# Percentuali di ciascun cluster\n",
        "def print_cluster_stats(labels, name):\n",
        "    unique, counts = np.unique(labels, return_counts=True)\n",
        "    total = counts.sum()\n",
        "    print(f\"{name}:\")\n",
        "    for u, c in zip(unique, counts):\n",
        "        print(f\"  Cluster {u}: {c} ({c / total * 100:.2f}%)\")\n",
        "    print()\n",
        "\n",
        "print_cluster_stats(labels_low, f\"Test LOW ({frac_anom_low:.3f}% di anomalie MSE)\")\n",
        "print_cluster_stats(labels_high, f\"Test HIGH ({frac_anom_high:.3f}% di anomalie MSE)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OUZW4knbkKW"
      },
      "source": [
        "**Analisi combinata: Errore di ricostruzione vs Cluster GMM**\n",
        "\n",
        "Come approfondimento, abbiamo confrontato la distribuzione dell'**errore di ricostruzione MSE** con le **etichette di clustering GMM** per i dataset **Test LOW** e **Test HIGH**.\n",
        "\n",
        "Nella prima riga del grafico vengono mostrati gli istogrammi complessivi degli errori MSE, sovrapposti alla soglia di anomalia fissata al 90° percentile del validation set. La seconda riga invece mostra la **stessa distribuzione suddivisa per cluster GMM**, evidenziando chiaramente come un cluster tenda a contenere la maggior parte degli outlier (cioè degli eventi con errore maggiore della soglia).\n",
        "\n",
        "Questa sovrapposizione suggerisce che il clustering nello spazio latente del modello abbia effettivamente appreso una separazione informativa tra eventi normali e anomali, **coerente con l'anomaly score basato sull'errore di ricostruzione**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzgiDpWDyaS1"
      },
      "outputs": [],
      "source": [
        "#@title (TASK 4) Misura della purezza dei cluster rispetto alla classificazione fatta con l'MSE\n",
        "\n",
        "def purity_score(cluster_labels, anomaly_labels):\n",
        "    N = len(cluster_labels)\n",
        "    purity_sum = 0\n",
        "    for cl in np.unique(cluster_labels):\n",
        "        idx = (cluster_labels == cl)\n",
        "        # Quanti punti \"anomali\" e \"normali\" nel cluster\n",
        "        counts = np.bincount(anomaly_labels[idx], minlength=2)\n",
        "        purity_sum += counts.max()\n",
        "    return purity_sum / N\n",
        "\n",
        "# 1. Calcola anomaly labels\n",
        "anomaly_labels_low = (test_errors_l < threshold).astype(int)   # 1 = normale\n",
        "anomaly_labels_high = (test_errors_h < threshold).astype(int)\n",
        "\n",
        "# 2. Calcola purity per LOW e HIGH\n",
        "purity_low = purity_score(labels_low, anomaly_labels_low)\n",
        "purity_high = purity_score(labels_high, anomaly_labels_high)\n",
        "\n",
        "print(f\"Purity nei cluster:\")\n",
        "print(f\"  Test LOW : {purity_low:.3f}\")\n",
        "print(f\"  Test HIGH: {purity_high:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUvA1HyucE0W"
      },
      "source": [
        "**TASK 4 – Purezza dei cluster rispetto alle anomalie**\n",
        "\n",
        "Per valutare la coerenza tra le **etichette di cluster** assegnate dal GMM e la **classificazione anomalia vs normale** determinata tramite soglia sull'errore MSE, abbiamo calcolato la **purity** dei cluster.\n",
        "\n",
        "La *purity* misura, per ogni cluster, la frazione di elementi appartenenti alla classe più rappresentata (in questo caso, eventi normali o anomali secondo l’MSE). È definita come:\n",
        "\n",
        "$$\n",
        "\\text{Purity} = \\frac{1}{N} \\sum_{k} \\max_{j} |c_k \\cap t_j|\n",
        "$$\n",
        "\n",
        "dove:\n",
        "- $N$ è il numero totale di esempi,\n",
        "- $c_k$ è l’insieme degli esempi nel cluster $k$,\n",
        "- $t_j$ è l’insieme degli esempi con etichetta $j$ (anomalo o normale).\n",
        "\n",
        "Abbiamo calcolato la purity separatamente per **Test LOW** e **Test HIGH**, utilizzando le label derivate dal threshold MSE sul validation set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kifA-LR6RzSz"
      },
      "source": [
        "### Modello L -> 32D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt01G6a1j75U"
      },
      "outputs": [],
      "source": [
        "model_L = ConvAE(params_L).to(DEVICE)\n",
        "\n",
        "# Supponendo input (1, 100, 100)\n",
        "summary(model_L, input_size=(1, 100, 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_aJMCX5R7sV"
      },
      "outputs": [],
      "source": [
        "#@title Training del modello Large\n",
        "\n",
        "# Inizializza il modello\n",
        "model_L = ConvAE(params_L)\n",
        "\n",
        "early_stop_cb = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=PATIENCE,\n",
        "    mode=\"min\"\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    max_epochs=MAX_EPOCHS,\n",
        "    accelerator=\"auto\",\n",
        "    callbacks=[early_stop_cb],\n",
        "    log_every_n_steps=10\n",
        ")\n",
        "\n",
        "# Training\n",
        "trainer.fit(model_L, train_loader, val_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqlIsQzF0ZlQ"
      },
      "outputs": [],
      "source": [
        "#@title Confronto degli errori di ricostruzione: Test LOW, Test HIGH\n",
        "\n",
        "def compute_reconstruction_errors(model, dataloader):\n",
        "    model.eval()\n",
        "    errors = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            x = batch[0].to(model.device)\n",
        "            x_hat = model(x)\n",
        "            mse = F.mse_loss(x_hat, x, reduction='none')\n",
        "            mse_per_image = mse.view(mse.size(0), -1).mean(dim=1)\n",
        "            errors.extend(mse_per_image.cpu().numpy())\n",
        "    return np.array(errors)\n",
        "\n",
        "# Calcolo errori\n",
        "train_errors = compute_reconstruction_errors(model_L, train_loader)\n",
        "val_errors = compute_reconstruction_errors(model_L, val_loader)\n",
        "test_errors_l = compute_reconstruction_errors(model_L, test_loader_l)\n",
        "test_errors_h = compute_reconstruction_errors(model_L, test_loader_h)\n",
        "\n",
        "print(f\"Train MSE: {np.mean(train_errors):.7f} | Val MSE: {np.mean(val_errors):.7f}\")\n",
        "print(f\"Test LOW MSE: {np.mean(test_errors_l):.7f} | Test HIGH MSE: {np.mean(test_errors_h):.7f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZSFiMJv2uyn"
      },
      "outputs": [],
      "source": [
        "#@title (TASK 1) Plot 3D (Riduzione da 32D) delle rappresentazioni latenti di: Train+Val, Test LOW, Test HIGH\n",
        "\n",
        "# Estrazione\n",
        "latents_train = extract_latents(train_loader, model_L)\n",
        "latents_val = extract_latents(val_loader, model_L)\n",
        "latents_low = extract_latents(test_loader_l, model_L)\n",
        "latents_high = extract_latents(test_loader_h, model_L)\n",
        "\n",
        "# Standardizzazione\n",
        "scaler = StandardScaler()\n",
        "latents_train_std = scaler.fit_transform(latents_train)\n",
        "latents_val_std = scaler.transform(latents_val)\n",
        "latents_low_std = scaler.transform(latents_low)\n",
        "latents_high_std = scaler.transform(latents_high)\n",
        "\n",
        "# PCA → 3D\n",
        "pca = PCA(n_components=3, random_state=SEED)\n",
        "latents_train_pca = pca.fit_transform(latents_train)\n",
        "latents_val_pca = pca.transform(latents_val)\n",
        "latents_low_pca = pca.transform(latents_low)\n",
        "latents_high_pca = pca.transform(latents_high)\n",
        "\n",
        "# Subplots Plotly\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=3,\n",
        "    specs=[[{'type': 'scatter3d'}]*3],\n",
        "    subplot_titles=[\"Train\", \"Test LOW\", \"Test HIGH\"]\n",
        ")\n",
        "\n",
        "# Tracce\n",
        "fig.add_trace(plot_latents_plotly(latents_train_pca, \"Train\", \"deepskyblue\"), row=1, col=1)\n",
        "fig.add_trace(plot_latents_plotly(latents_val_pca, \"Val\", \"steelblue\"), row=1, col=1)\n",
        "fig.add_trace(plot_latents_plotly(latents_low_pca, \"Test LOW\", \"orange\"), row=1, col=2)\n",
        "fig.add_trace(plot_latents_plotly(latents_high_pca, \"Test HIGH\", \"purple\"), row=1, col=3)\n",
        "\n",
        "# Layout\n",
        "fig.update_layout(\n",
        "    height=600, width=1800,\n",
        "    title_text=\"Spazio Latente (PCA) - Train+Val vs Test\",\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZWkAaVBc_Ev"
      },
      "source": [
        "**TASK 1 – Visualizzazione dello spazio latente (Model L)**\n",
        "\n",
        "**Confronto con Model S:**  \n",
        "Nel caso del Model L è stato necessario effettuare riduzione dimensionale a causa delle 32 dimensioni. Tuttavia, l'uso di un modello con maggiore capacità espressiva come il Model L consente una separazione latente più sofisticata e potenzialmente più informativa, anche se a costo di una maggiore complessità nella visualizzazione.\n",
        "\n",
        "Possiamo notare nei plot che i punti sono distribuiti più uniformemente nello spazio rispetto a al plot della TASK 1 con il Model S.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4R8kG0la5ECb"
      },
      "outputs": [],
      "source": [
        "#@title (TASK 2) Stima delle anomalie nei Test LOW e Test HIGH (tramite errore MSE)\n",
        "\n",
        "threshold = np.percentile(val_errors, 90)\n",
        "print(f\"Soglia impostata (90° percentile val set): {threshold:.6f}\")\n",
        "\n",
        "# Stima della frazione anomala\n",
        "frac_anom_low = np.mean(test_errors_l > threshold)\n",
        "frac_anom_high = np.mean(test_errors_h > threshold)\n",
        "\n",
        "# Plot\n",
        "fig, axs = plt.subplots(1, 3, figsize=(20, 5), sharey=True, sharex=True)\n",
        "datasets = [val_errors, test_errors_l, test_errors_h]\n",
        "titles = [\"Validation\", \"Test LOW\", \"Test HIGH\"]\n",
        "colors = ['steelblue', 'orange', 'purple']\n",
        "\n",
        "for ax, data, title, color in zip(axs, datasets, titles, colors):\n",
        "    n, bins, patches = ax.hist(data, bins=100, color=color)\n",
        "    ax.axvline(threshold, color='red', linestyle='--', label='Soglia 90° percentile')\n",
        "    for patch, left in zip(patches, bins[:-1]):\n",
        "        if left >= threshold:\n",
        "            patch.set_edgecolor('red')\n",
        "            patch.set_linewidth(2)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"MSE\")\n",
        "    ax.legend()\n",
        "\n",
        "plt.suptitle(\"Distribuzione degli errori di ricostruzione (MSE) per immagine\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFrazione stimata di anomalie:\")\n",
        "print(f\"Test LOW : {frac_anom_low:.3f}\")\n",
        "print(f\"Test HIGH: {frac_anom_high:.3f}\")\n",
        "print(f\"Verifica relazione: f_high > f_low → {frac_anom_high > frac_anom_low}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWr0avae5XZA"
      },
      "outputs": [],
      "source": [
        "#@title (TASK 3) Clustering delle rappresentazioni latenti con GMM\n",
        "\n",
        "\"\"\"# Funzione per rietichettare i cluster in base all'errore medio\n",
        "def reorder_gmm_labels(errors, labels):\n",
        "    cluster_means = [errors[labels == i].mean() for i in np.unique(labels)]\n",
        "    new_order = np.argsort(cluster_means)\n",
        "    label_map = {old: new for new, old in enumerate(new_order)}\n",
        "    return np.vectorize(label_map.get)(labels)\"\"\"\n",
        "\n",
        "# GMM clustering\n",
        "# Concatenazione dei dati\n",
        "latents_combined = np.concatenate([latents_low_std, latents_high_std], axis=0)\n",
        "\n",
        "# Fit del GMM su entrambi\n",
        "gmm = GaussianMixture(n_components=2, random_state=SEED)\n",
        "gmm.fit(latents_combined)\n",
        "\n",
        "# Predizione separata mantenendo coerenza dei cluster\n",
        "labels_low = gmm.predict(latents_low_std)\n",
        "labels_high = gmm.predict(latents_high_std)\n",
        "\n",
        "\n",
        "\"\"\"# Rietichettatura coerente\n",
        "labels_low = reorder_gmm_labels(test_errors_l, labels_low)\n",
        "labels_high = reorder_gmm_labels(test_errors_h, labels_high)\"\"\"\n",
        "\n",
        "# Plot interattivo\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=2,\n",
        "    specs=[[{'type': 'scatter3d'}, {'type': 'scatter3d'}]],\n",
        "    subplot_titles=[\"GMM Clustering - Test LOW\", \"GMM Clustering - Test HIGH\"]\n",
        ")\n",
        "\n",
        "fig.add_trace(go.Scatter3d(\n",
        "    x=latents_low_pca[:, 0], y=latents_low_pca[:, 1], z=latents_low_pca[:, 2],\n",
        "    mode='markers',\n",
        "    marker=dict(size=2, color=labels_low, colorscale='Viridis', opacity=0.6),\n",
        "    name=\"LOW\"\n",
        "), row=1, col=1)\n",
        "\n",
        "fig.add_trace(go.Scatter3d(\n",
        "    x=latents_high_pca[:, 0], y=latents_high_pca[:, 1], z=latents_high_pca[:, 2],\n",
        "    mode='markers',\n",
        "    marker=dict(size=2, color=labels_high, colorscale='Viridis', opacity=0.6),\n",
        "    name=\"HIGH\"\n",
        "), row=1, col=2)\n",
        "\n",
        "fig.update_layout(\n",
        "    height=600, width=1000,\n",
        "    title_text=\"GMM Clustering nello Spazio Latente\",\n",
        "    showlegend=False\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NanraJHKetBT"
      },
      "source": [
        "**Task 3 – Clustering nello spazio latente (Model L)**\n",
        "\n",
        "A differenza del modello **S**, dove lo spazio latente è già tridimensionale, nel caso del modello **L** l’uso del PCA si è rivelato cruciale per evidenziare una struttura più regolare e diffusa dei dati. La maggiore espressività dello spazio latente del modello L consente al GMM di individuare cluster visivamente più separabili.\n",
        "\n",
        "Nella figura riportata, si osserva una discreta separazione tra i due cluster sia nel test set LOW che in quello HIGH. Questo suggerisce che le rappresentazioni latenti contengono informazioni sufficientemente discriminanti per supportare il clustering non supervisionato.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoNMYn7Q58Be"
      },
      "outputs": [],
      "source": [
        "#@title Errore di Ricostruzione VS Clustering dei Test Set\n",
        "\n",
        "fig, axs = plt.subplots(2, 2, figsize=(12, 8), sharex=True, sharey=True)\n",
        "\n",
        "# Prima riga: errori complessivi\n",
        "datasets = [test_errors_l, test_errors_h]\n",
        "titles = [\"Test LOW - Anomaly Score\", \"Test HIGH - Anomaly Score\"]\n",
        "colors = ['orange', 'purple']\n",
        "\n",
        "for ax, data, title, color in zip(axs[0], datasets, titles, colors):\n",
        "    n, bins, patches = ax.hist(data, bins=50, color=color)\n",
        "    ax.axvline(threshold, color='red', linestyle='--', label='Soglia 90° percentile')\n",
        "    for patch, left in zip(patches, bins[:-1]):\n",
        "        if left >= threshold:\n",
        "            patch.set_edgecolor('red')\n",
        "            patch.set_linewidth(2)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"MSE\")\n",
        "    ax.legend()\n",
        "\n",
        "# Seconda riga: divisione per cluster\n",
        "cluster_info = [\n",
        "    (test_errors_l, labels_low, \"Test LOW - Cluster\", ['goldenrod', 'orangered']),\n",
        "    (test_errors_h, labels_high, \"Test HIGH - Cluster\", ['indigo', 'mediumorchid'])\n",
        "]\n",
        "\n",
        "for ax, (errors, labels, title_prefix, cluster_colors) in zip(axs[1], cluster_info):\n",
        "    for cluster_id, color in zip([0, 1], cluster_colors):\n",
        "        cluster_errors = errors[labels == cluster_id]\n",
        "        ax.hist(cluster_errors, bins=50, alpha=0.7, label=f\"{title_prefix} {cluster_id}\", color=color)\n",
        "    ax.axvline(threshold, color='red', linestyle='--', label='Soglia 90° percentile')\n",
        "    ax.set_xlabel(\"MSE\")\n",
        "    ax.legend()\n",
        "    ax.set_title(f\"{title_prefix} - Anomaly Score by Cluster\")\n",
        "\n",
        "plt.suptitle(\"Distribuzione degli errori di ricostruzione (MSE) e divisione per cluster\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()\n",
        "\n",
        "print()\n",
        "\n",
        "print_cluster_stats(labels_low, f\"Test LOW ({frac_anom_low:.3f}% di anomalie MSE)\")\n",
        "print_cluster_stats(labels_high, f\"Test HIGH ({frac_anom_high:.3f}% di anomalie MSE)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YDEDsp1e0bB"
      },
      "source": [
        "**Confronto GMM vs MSE per i due test set**\n",
        "\n",
        "\n",
        "Il confronto evidenzia che:\n",
        "- Per entrambi i test set, i due cluster hanno distribuzioni dell’errore MSE significativamente diverse.\n",
        "- Il clustering sullo spazio latente generato dal modello **L** permette una separazione più netta tra le modalità, con istogrammi bimodali evidenti, rispetto al modello **S**, dove le distribuzioni si sovrapponevano maggiormente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_u1zwP86Oyg"
      },
      "outputs": [],
      "source": [
        "#@title (TASK 4) Misura della purezza dei cluster rispetto alla classificazione fatta con l'MSE\n",
        "\n",
        "# 1. Calcola anomaly labels\n",
        "anomaly_labels_low = (test_errors_l > threshold).astype(int)   # 1 = anomalia\n",
        "anomaly_labels_high = (test_errors_h > threshold).astype(int)\n",
        "\n",
        "# 2. Calcola purity per LOW e HIGH\n",
        "purity_low = purity_score(labels_low, anomaly_labels_low)\n",
        "purity_high = purity_score(labels_high, anomaly_labels_high)\n",
        "\n",
        "print(f\"Purity nei cluster:\")\n",
        "print(f\"  Test LOW : {purity_low:.3f}\")\n",
        "print(f\"  Test HIGH: {purity_high:.3f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
