{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mich1803/AnomalyDetection-AE-LHC/blob/main/AD_AE_LHC_PhysAI2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Group Project 2024/25\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8ix-CMEfVfV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task\n"
      ],
      "metadata": {
        "id": "Fs3lL8NouR0R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Allenare un AE per Anomaly Detection in grado di riconoscere segnali prodotti da jeti adronici anomali in un reivelatore di fisica delle alte energie.\n",
        "\n",
        "Alle energie estreme del Large Hadron Collider, particelle massive possono essere prodotte con un tale boost di Lorentz da far sì che i loro decadimenti in adroni (getti adronici) risultino così collimati che le particelle prodotte si sovrappongono. Determinare se la sottostruttura di un getto osservato sia dovuta a una singola particella di bassa massa oppure a molteplici prodotti di decadimento di una particella di massa elevata è un problema cruciale nell’analisi dei dati del LHC. Gli approcci tradizionali si basano su osservabili di alto livello costruite a partire da modelli teorici di deposizione di energia nei calorimetri e da parametri delle tracce cariche ricostruite nel tracciatore interno, ma la complessità dei dati rende questo compito un candidato ideale per l’applicazione di strumenti di deep learning. I costituenti dei getti possono infatti essere rappresentati come immagini 2D in cui ogni pixel rappresenta una delle celle sensibili del calorimetro, e il contenuto della cella una misura dell'energia o della quantità di moto depositata nella cella.\n",
        "\n",
        "**Dataset:**\n",
        "\n",
        "I dati del progetto sono nella forma di immagini 2D di dimensione (100,100), ogni cella rappresenta l'energia depositata in quella cella dalle particelle del jet adronico corrispondente. Ci sono due tipologie di jet adronici consider ati: *jet normali*, costituiti dalla adronizzazione di un quark leggero o gluone, e *jet anomali* (disponibili in una frazione incognita solo nel test set) costituiti dall'adronizzazione dei quark nel decadimento $t \\to Wb \\to qq'b$, in cui a causa del boost del quark top, i tre quark nello stato finale sono parzialmente sovrapposti.\n",
        "\n",
        "* *Normal data dataset:* 12k jet rappresentati come histogrammi 2D della quantità di moto depositata in ciascuno dei 100x100 bin di una finestra quadrata nel piano ($\\theta,\\phi$) centrato intorno all'asse del jet.\n",
        "\n",
        "* *Test dataset:*\n",
        "due dataset costituiti ciascuno da 3k eventi, contenenti jet normali e jet anomali in una frazione relativa icognita da determinare. Nel primo dataset (*_high*) la frazione incognita di eventi anomali è $\\ge 55\\%$. Nel secondo dataset (*_low*) la frazione incognita di eventi anomali incognita è $\\le 45\\%$.\n",
        "Potete utilizzare questa informazione per verificare che le vostre predizioni soddisfino la relazione $f_{high} > f_{low}$.\n",
        "\n",
        "I dati sono forniti come array numpy in un file numpy compresso (.npz), leggibile con l'esempio di codice che segue:\n",
        "\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "\n",
        "f_train = np.load('Normal_data.npz')\n",
        "f_test_l = np.load('Test_data_low.npz')\n",
        "f_test_h = np.load('Test_data_high.npz')\n",
        "\n",
        "normal_data = f_train['normal_data']\n",
        "test_data_l = f_test_l['test_data']\n",
        "test_data_h = f_test_h['test_data']\n",
        "\n",
        "print(normal_data.shape)\n",
        "print(test_data_l.shape)\n",
        "print(test_data_h.shape)\n",
        "```\n",
        "\n",
        "**Per scaricare i dataset:**\n",
        "* dati normali:\n",
        "```\n",
        "!wget http://giagu.web.cern.ch/giagu/CERN/P2025/Normal_data.npz\n",
        "```\n",
        "* dati anomali:\n",
        "```\n",
        "!wget http://giagu.web.cern.ch/giagu/CERN/P2025/<Identificativo Dataset>/Test_data_low.npz\n",
        "!wget http://giagu.web.cern.ch/giagu/CERN/P2025/<Identificativo Dataset>/Test_data_high.npz\n",
        "```\n",
        "```\n",
        "# <Identificativo Dataset> dal foglio excel prenotazione gruppi\n",
        "```\n",
        "\n",
        "\n",
        "**Obiettivi minimi del progetto (potete a vostro piacimento aggiungere ulteriori analisi/studi:**\n",
        "\n",
        "1. Plot della rappresentazione latente delle immagini di test fatto con riduzione dimensionale.\n",
        "2. Stima della frazione di eventi anomali presente nei due Test dataset, tenendo conto che la di procedura di stima deve garantire che la rate di falsi postivi sia inferiore a circa il $10\\%$ (FPR $\\le \\sim 10\\%$).\n",
        "3. Clustering dello spazio (per esempio usando un algoritmo GMM).\n",
        "4. Misura della purezza dei cluster rispetto alle label assegnate in anomaly score.\n",
        "\n",
        "\n",
        "**Nota Importante:**\n",
        "\n",
        "Il notebook deve essere compilato come una relazione scientifica del progetto, quindi deve contenere sia il codice (leggibile e riproducibile), i risultati in termini di grafici e tabelle numeriche, e il testo che illustra la strategia ottenuta, le scelte compiute, e i risultati ottenuti."
      ],
      "metadata": {
        "id": "ZtrtaHDwuV_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduzione"
      ],
      "metadata": {
        "id": "GHD01QBg2Z3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In questo progetto proponiamo un approccio basato su Autoencoder convolutivi per l'identificazione di anomalie nei segnali prodotti da jet adronici in un rivelatore del Large Hadron Collider. Le immagini utilizzate rappresentano mappe bidimensionali della quantità di moto depositata nei calorimetri, e vengono trattate come input per reti neurali convolutive.\n",
        "\n",
        "L'obiettivo principale è confrontare due strategie per la rilevazione di anomalie:\n",
        "1. Una basata sull'errore di ricostruzione (Mean Squared Error, MSE) tra immagine originale e immagine ricostruita;\n",
        "2. Una basata su un'analisi di clustering dello spazio latente (ottenuto tramite codifica dell'Autoencoder), in particolare utilizzando un algoritmo Gaussian Mixture Model (GMM).\n",
        "\n",
        "Per svolgere tale confronto sono stati addestrati due modelli di Autoencoder con architettura convolutiva:\n",
        "\n",
        "- **Modello S**: spazio latente a 3 dimensioni, direttamente visualizzabile in 3D.\n",
        "- **Modello L**: spazio latente a 32 dimensioni, successivamente ridotto a 3D tramite PCA per scopi di visualizzazione e clustering.\n",
        "\n",
        "Per entrambi i modelli, l'addestramento è stato effettuato unicamente su eventi normali, mentre il test è stato condotto su due dataset contenenti una frazione ignota di eventi anomali.\n",
        "\n",
        "**Autori**  \n",
        "Michele Magrini (2066963)  \n",
        "Julian Hendrix (Matricola)"
      ],
      "metadata": {
        "id": "JfgJcItSAKnI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Codice"
      ],
      "metadata": {
        "id": "jwlpgVGjuarB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import librerie, settaggio seed per la riproducibilità\n",
        "!pip install -q pytorch-lightning\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn.functional as F\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import plotly.graph_objs as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Impostazioni per la riproducibilità\n",
        "SEED = 42\n",
        "pl.seed_everything(SEED, workers=True)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "\n",
        "# Device (CPU/GPU)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {DEVICE} device\")\n",
        "\n",
        "# Download dei dataset\n",
        "ID = \"G15\"\n",
        "!wget -nc http://giagu.web.cern.ch/giagu/CERN/P2025/Normal_data.npz\n",
        "!wget -nc http://giagu.web.cern.ch/giagu/CERN/P2025/{ID}/Test_data_low.npz\n",
        "!wget -nc http://giagu.web.cern.ch/giagu/CERN/P2025/{ID}/Test_data_high.npz"
      ],
      "metadata": {
        "id": "gKykx92QJO18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Caricamento dei Dataset/Dataloader e visualizzazione preliminare"
      ],
      "metadata": {
        "id": "AKQgMI5O0WZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Creazione Datasets e DataLoader PyTorch\n",
        "\n",
        "f_train = np.load('Normal_data.npz')\n",
        "f_test_l = np.load('Test_data_low.npz')\n",
        "f_test_h = np.load('Test_data_high.npz')\n",
        "\n",
        "normal_data = f_train['normal_data']\n",
        "test_data_l = f_test_l['test_data']\n",
        "test_data_h = f_test_h['test_data']\n",
        "\n",
        "# Normalizzazione [0,1] usando solo il training set\n",
        "train_min = np.min(normal_data)\n",
        "train_max = np.max(normal_data)\n",
        "\n",
        "def normalize(data, data_min, data_max):\n",
        "    return (data - data_min) / (data_max - data_min + 1e-8)\n",
        "\n",
        "normal_data = normalize(normal_data, train_min, train_max)\n",
        "test_data_l = normalize(test_data_l, train_min, train_max)\n",
        "test_data_h = normalize(test_data_h, train_min, train_max)\n",
        "\n",
        "# Conversione in tensori torch\n",
        "normal_tensor = torch.tensor(normal_data, dtype=torch.float32).unsqueeze(1)\n",
        "test_tensor_l = torch.tensor(test_data_l, dtype=torch.float32).unsqueeze(1)\n",
        "test_tensor_h = torch.tensor(test_data_h, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Split: 90% train, 10% val\n",
        "train_len = int(0.9 * len(normal_tensor))\n",
        "val_len = len(normal_tensor) - train_len\n",
        "full_dataset = TensorDataset(normal_tensor)\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_len, val_len])\n",
        "\n",
        "# Dataset\n",
        "test_dataset_l = TensorDataset(test_tensor_l)\n",
        "test_dataset_h = TensorDataset(test_tensor_h)\n",
        "\n",
        "# DataLoader\n",
        "BATCH_SIZE = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader_l = DataLoader(test_dataset_l, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader_h = DataLoader(test_dataset_h, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Info\n",
        "print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test Low: {len(test_dataset_l)} | Test High: {len(test_dataset_h)}\")\n",
        "# Print Sample Shape\n",
        "print(f\"Train Sample Shape: {train_dataset[0][0].shape}\")\n",
        "\n",
        "datasets = {\n",
        "    \"Train (Normal)\": normal_data,\n",
        "    \"Test Low\": test_data_l,\n",
        "    \"Test High\": test_data_h\n",
        "}\n",
        "print(\"\\nLa normalizzazione tra 0 e 1 è basata sui dati di training:\")\n",
        "for name, data in datasets.items():\n",
        "    print(f\"{name}: min = {np.min(data):.4f}, max = {np.max(data):.4f}\")\n"
      ],
      "metadata": {
        "id": "qsbJ-Pb_a_og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title plot di alcuni eventi del dataset\n",
        "\n",
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(10, 10))\n",
        "fig.suptitle(\"Primi 3 esempi per dataset (100x100)\\nScala dei colori locale per ciascuna immagine\", fontsize=14)\n",
        "\n",
        "for row_idx, (label, data) in enumerate(datasets.items()):\n",
        "    for col_idx in range(3):\n",
        "        ax = axes[row_idx, col_idx]\n",
        "        img = data[col_idx]\n",
        "        im = ax.imshow(img, cmap=\"inferno\", origin=\"lower\", vmin=np.min(img), vmax=np.max(img))\n",
        "        ax.axis(\"off\")\n",
        "        if col_idx == 0:\n",
        "            ax.set_title(label, fontsize=12, loc='left')\n",
        "\n",
        "fig.subplots_adjust(left=0.05, right=0.95, top=0.9, bottom=0.05, wspace=0.1, hspace=0.2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ADkE7_elwPBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definizione del Modello e dei Parametri\n"
      ],
      "metadata": {
        "id": "47ZfbzL884h-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Modello ConvAE\n",
        "\n",
        "class ConvAE(pl.LightningModule):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(config)\n",
        "        self.lr = config.get(\"lr\", 1e-3)\n",
        "        self.dropout = config.get(\"dropout\", 0.0)\n",
        "        hidden_dim = config[\"hidden_dim\"]\n",
        "\n",
        "        # Encoder\n",
        "        in_channels = 1\n",
        "        encoder_layers = []\n",
        "        for conv in config[\"in_conv\"]:\n",
        "            encoder_layers.append(nn.Conv2d(in_channels, **{k: v for k, v in conv.items() if k != \"output_padding\"}))\n",
        "            encoder_layers.append(nn.ReLU())\n",
        "            if self.dropout > 0:\n",
        "                encoder_layers.append(nn.Dropout2d(self.dropout))\n",
        "            in_channels = conv[\"out_channels\"]\n",
        "        self.encoder_conv = nn.Sequential(*encoder_layers)\n",
        "\n",
        "        # Calcola shape del bottleneck convoluzionale\n",
        "        dummy_input = torch.zeros(1, 1, 100, 100)\n",
        "        with torch.no_grad():\n",
        "            dummy_out = self.encoder_conv(dummy_input)\n",
        "            self.conv_shape = dummy_out.shape[1:]  # (C, H, W)\n",
        "            self.flat_dim = dummy_out.numel()\n",
        "\n",
        "\n",
        "        # Bottleneck FC\n",
        "        self.encoder_fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(self.flat_dim, hidden_dim)\n",
        "        )\n",
        "        self.decoder_fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, self.flat_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        decoder_layers = []\n",
        "        in_channels = config[\"in_conv\"][-1][\"out_channels\"]\n",
        "        for conv in config[\"out_conv\"]:\n",
        "            decoder_layers.append(nn.ConvTranspose2d(in_channels, **conv))\n",
        "            decoder_layers.append(nn.ReLU())\n",
        "            if self.dropout > 0:\n",
        "                decoder_layers.append(nn.Dropout2d(self.dropout))\n",
        "            in_channels = conv[\"out_channels\"]\n",
        "        decoder_layers[-2] = nn.Sigmoid()  # ultima attivazione\n",
        "        self.decoder_conv = nn.Sequential(*decoder_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder_conv(x)\n",
        "        x = self.encoder_fc(x)\n",
        "        x = self.decoder_fc(x)\n",
        "        x = x.view(-1, *self.conv_shape)  # usa shape salvata\n",
        "        x = self.decoder_conv(x)\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x = batch[0]\n",
        "        x_hat = self(x)\n",
        "        loss = F.mse_loss(x_hat, x)\n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x = batch[0]\n",
        "        x_hat = self(x)\n",
        "        loss = F.mse_loss(x_hat, x)\n",
        "        self.log(\"val_loss\", loss, prog_bar=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n"
      ],
      "metadata": {
        "id": "oIPwVTGq89-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Parametri S e L\n",
        "\n",
        "# Small (3D spazio latente)\n",
        "params_S = {\n",
        "    \"hidden_dim\": 3,\n",
        "    \"in_conv\": [\n",
        "        {\"out_channels\": 16, \"kernel_size\": 4, \"stride\": 2, \"padding\": 1},  # 100 → 50\n",
        "        {\"out_channels\": 32, \"kernel_size\": 4, \"stride\": 2, \"padding\": 1},  # 50 → 25\n",
        "        {\"out_channels\": 64, \"kernel_size\": 4, \"stride\": 2, \"padding\": 1}   # 25 → 13\n",
        "    ],\n",
        "    \"out_conv\": [\n",
        "        {\"out_channels\": 32, \"kernel_size\": 4, \"stride\": 2, \"padding\": 1, \"output_padding\": 1},  # 13 → 25\n",
        "        {\"out_channels\": 16, \"kernel_size\": 4, \"stride\": 2, \"padding\": 1, \"output_padding\": 0},  # 25 → 50\n",
        "        {\"out_channels\": 1,  \"kernel_size\": 4, \"stride\": 2, \"padding\": 1, \"output_padding\": 0}   # 50 → 100\n",
        "    ],\n",
        "    \"dropout\": 0.2,\n",
        "    \"lr\": 1e-3\n",
        "}\n",
        "\n",
        "# Large (32D spazio latente)\n",
        "params_L = {**params_S, \"hidden_dim\": 32}\n",
        "\n",
        "MAX_EPOCHS = 50\n",
        "PATIENCE = MAX_EPOCHS // 5\n"
      ],
      "metadata": {
        "id": "RxFOPJ_f-cpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modello S -> 3D"
      ],
      "metadata": {
        "id": "J3F04lVy_jUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inizializza il modello\n",
        "from torchsummary import summary\n",
        "model_S = ConvAE(params_S).to(DEVICE)\n",
        "\n",
        "summary(model_S, input_size=(1, 100, 100))"
      ],
      "metadata": {
        "id": "qAcAG8oOisFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training del modello Small\n",
        "\n",
        "model_S = ConvAE(params_S)\n",
        "\n",
        "early_stop_cb = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=PATIENCE,\n",
        "    mode=\"min\"\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    max_epochs=MAX_EPOCHS,\n",
        "    accelerator=\"auto\",\n",
        "    callbacks=[early_stop_cb],\n",
        "    log_every_n_steps=10\n",
        ")\n",
        "\n",
        "# Training\n",
        "trainer.fit(model_S, train_loader, val_loader)\n"
      ],
      "metadata": {
        "id": "x9G4hl5f_gjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Confronto degli errori di ricostruzione: Test LOW, Test HIGH\n",
        "\n",
        "def compute_reconstruction_errors(model, dataloader):\n",
        "    model.eval()\n",
        "    errors = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            x = batch[0].to(model.device)\n",
        "            x_hat = model(x)\n",
        "            mse = F.mse_loss(x_hat, x, reduction='none')\n",
        "            mse_per_image = mse.view(mse.size(0), -1).mean(dim=1)\n",
        "            errors.extend(mse_per_image.cpu().numpy())\n",
        "    return np.array(errors)\n",
        "\n",
        "# Calcolo errori\n",
        "train_errors = compute_reconstruction_errors(model_S, train_loader)\n",
        "val_errors = compute_reconstruction_errors(model_S, val_loader)\n",
        "test_errors_l = compute_reconstruction_errors(model_S, test_loader_l)\n",
        "test_errors_h = compute_reconstruction_errors(model_S, test_loader_h)\n",
        "\n",
        "print(f\"Train MSE: {np.mean(train_errors):.7f} | Val MSE: {np.mean(val_errors):.7f}\")\n",
        "print(f\"Test LOW MSE: {np.mean(test_errors_l):.7f} | Test HIGH MSE: {np.mean(test_errors_h):.7f}\")"
      ],
      "metadata": {
        "id": "HclMfdNfDRD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (TASK 1) Plot 3D delle rappresentazioni latenti di: Train+Val, Test LOW, Test HIGH\n",
        "\n",
        "def extract_latents(dataloader, model):\n",
        "    latents = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            x = batch[0].to(model.device)\n",
        "            z = model.encoder_fc(model.encoder_conv(x))\n",
        "            latents.append(z.cpu().numpy())\n",
        "    return np.concatenate(latents, axis=0)\n",
        "\n",
        "# Estrazione\n",
        "latents_train = extract_latents(train_loader, model_S)\n",
        "latents_val = extract_latents(val_loader, model_S)\n",
        "latents_low = extract_latents(test_loader_l, model_S)\n",
        "latents_high = extract_latents(test_loader_h, model_S)\n",
        "\n",
        "# Funzione per creare uno scatter3D Plotly\n",
        "def plot_latents_plotly(latents, name, color):\n",
        "    return go.Scatter3d(\n",
        "        x=latents[:, 0], y=latents[:, 1], z=latents[:, 2],\n",
        "        mode='markers',\n",
        "        marker=dict(size=2, color=color, opacity=0.6),\n",
        "        name=name\n",
        "    )\n",
        "\n",
        "# Subplots Plotly\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=3,\n",
        "    specs=[[{'type': 'scatter3d'}]*3],\n",
        "    subplot_titles=[\"Train\", \"Test LOW\", \"Test HIGH\"]\n",
        ")\n",
        "\n",
        "# Tracce\n",
        "fig.add_trace(plot_latents_plotly(latents_train, \"Train\", \"deepskyblue\"), row=1, col=1)\n",
        "fig.add_trace(plot_latents_plotly(latents_val, \"Val\", \"steelblue\"), row=1, col=1)\n",
        "fig.add_trace(plot_latents_plotly(latents_low, \"Test LOW\", \"orange\"), row=1, col=2)\n",
        "fig.add_trace(plot_latents_plotly(latents_high, \"Test HIGH\", \"purple\"), row=1, col=3)\n",
        "\n",
        "# Layout\n",
        "fig.update_layout(\n",
        "    height=600, width=1800,\n",
        "    title_text=\"Confronto Spazio Latente - Train+Val vs Test\",\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "pQ7rikRqNwDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizzazione dello Spazio Latente**\n",
        "\n",
        "Per visualizzare la rappresentazione appresa dallo spazio latente, abbiamo proiettato i dati del training set e dei due test set (LOW e HIGH) nello spazio latente di dimensione 3 utilizzando direttamente le coordinate apprese dall'autoencoder (modello `Small`).\n",
        "\n",
        "**Risultati e Interpretazione**\n",
        "\n",
        "- **Train**: I dati del training set (solo eventi normali) risultano fortemente concentrati su una regione molto compressa dello spazio latente.\n",
        "- **Test LOW & HIGH**: Anche i dati di test (contenenti anomalie) si distribuiscono lungo la stessa direzione latente. Non si osserva una separazione chiara tra eventi normali e anomali nello spazio latente a 3 dimensioni.\n",
        "\n",
        "Il fatto che i dati (train, test low e test high) risultino **allineati su una linea** suggerisce che:\n",
        "- Lo spazio latente appreso è **troppo compresso** (il modello ha appreso solo una variabilità principale).\n",
        "- Il modello riesce a **ricostruire bene anche gli eventi anomali**, rendendo difficile distinguere le anomalie solo dalla struttura latente.\n"
      ],
      "metadata": {
        "id": "0GR0P6YJWYp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (TASK 2) Stima delle anomalie nei Test LOW e Test HIGH (tramite errore MSE)\n",
        "\n",
        "threshold = np.percentile(val_errors, 90)\n",
        "print(f\"Soglia impostata (90° percentile val set): {threshold:.6f}\")\n",
        "\n",
        "# Stima della frazione anomala\n",
        "frac_anom_low = np.mean(test_errors_l > threshold)\n",
        "frac_anom_high = np.mean(test_errors_h > threshold)\n",
        "\n",
        "# Plot\n",
        "fig, axs = plt.subplots(1, 3, figsize=(20, 5), sharey=True, sharex=True)\n",
        "datasets = [val_errors, test_errors_l, test_errors_h]\n",
        "titles = [\"Validation\", \"Test LOW\", \"Test HIGH\"]\n",
        "colors = ['steelblue', 'orange', 'purple']\n",
        "\n",
        "for ax, data, title, color in zip(axs, datasets, titles, colors):\n",
        "    n, bins, patches = ax.hist(data, bins=100, color=color)\n",
        "    ax.axvline(threshold, color='red', linestyle='--', label='Soglia 90° percentile')\n",
        "    for patch, left in zip(patches, bins[:-1]):\n",
        "        if left >= threshold:\n",
        "            patch.set_edgecolor('red')\n",
        "            patch.set_linewidth(2)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"MSE\")\n",
        "    ax.legend()\n",
        "\n",
        "plt.suptitle(\"Distribuzione degli errori di ricostruzione (MSE) per immagine\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFrazione stimata di anomalie:\")\n",
        "print(f\"Test LOW : {frac_anom_low:.3f}\")\n",
        "print(f\"Test HIGH: {frac_anom_high:.3f}\")\n",
        "print(f\"Verifica relazione: f_high > f_low → {frac_anom_high > frac_anom_low}\")\n"
      ],
      "metadata": {
        "id": "MWrrzXJsW_N-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (TASK 3) Clustering delle rappresentazioni latenti con GMM\n",
        "\n",
        "\"\"\"# Funzione per rietichettare i cluster in base all'errore medio\n",
        "def reorder_gmm_labels(errors, labels):\n",
        "    unique_labels = np.unique(labels)\n",
        "    cluster_means = [errors[labels == i].mean() for i in unique_labels]\n",
        "    sorted_idx = np.argsort(cluster_means)\n",
        "    label_map = {unique_labels[sorted_idx[0]]: 0, unique_labels[sorted_idx[1]]: 1}\n",
        "    return np.vectorize(label_map.get)(labels)\"\"\"\n",
        "\n",
        "# Standardizzazione\n",
        "scaler = StandardScaler()\n",
        "latents_train_scaled = scaler.fit(latents_train)\n",
        "latents_low_scaled = scaler.transform(latents_low)\n",
        "latents_high_scaled = scaler.transform(latents_high)\n",
        "\n",
        "# Concatenazione dei dati\n",
        "latents_combined = np.concatenate([latents_low_scaled, latents_high_scaled], axis=0)\n",
        "\n",
        "# Fit del GMM su entrambi\n",
        "gmm = GaussianMixture(n_components=2, random_state=SEED)\n",
        "gmm.fit(latents_combined)\n",
        "\n",
        "# Predizione separata mantenendo coerenza dei cluster\n",
        "labels_low = gmm.predict(latents_low_scaled)\n",
        "labels_high = gmm.predict(latents_high_scaled)\n",
        "\n",
        "\"\"\"# Ordinamento delle etichette in base all'errore medio\n",
        "labels_low = reorder_gmm_labels(test_errors_l, labels_low)\n",
        "labels_high = reorder_gmm_labels(test_errors_h, labels_high)\"\"\"\n",
        "\n",
        "# Plot interattivo con subplots\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=2,\n",
        "    specs=[[{'type': 'scatter3d'}, {'type': 'scatter3d'}]],\n",
        "    subplot_titles=[\"GMM Clustering - Test LOW\", \"GMM Clustering - Test HIGH\"]\n",
        ")\n",
        "\n",
        "fig.add_trace(go.Scatter3d(\n",
        "    x=latents_low_scaled[:, 0], y=latents_low_scaled[:, 1], z=latents_low_scaled[:, 2],\n",
        "    mode='markers', marker=dict(size=2, color=labels_low, colorscale='Viridis', opacity=0.6)\n",
        "), row=1, col=1)\n",
        "\n",
        "fig.add_trace(go.Scatter3d(\n",
        "    x=latents_high_scaled[:, 0], y=latents_high_scaled[:, 1], z=latents_high_scaled[:, 2],\n",
        "    mode='markers', marker=dict(size=2, color=labels_high, colorscale='Viridis', opacity=0.6)\n",
        "), row=1, col=2)\n",
        "\n",
        "fig.update_layout(height=600, width=1000, title_text=\"GMM Clustering nello Spazio Latente\", showlegend=False)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "mgyNMny1t-M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Errore di Ricostruzione VS Clustering dei Test Set\n",
        "\n",
        "fig, axs = plt.subplots(2, 2, figsize=(12, 8), sharex=True, sharey=True)\n",
        "\n",
        "# Prima riga: errori complessivi\n",
        "datasets = [test_errors_l, test_errors_h]\n",
        "titles = [\"Test LOW - Anomaly Score\", \"Test HIGH - Anomaly Score\"]\n",
        "colors = ['orange', 'purple']\n",
        "\n",
        "for ax, data, title, color in zip(axs[0], datasets, titles, colors):\n",
        "    n, bins, patches = ax.hist(data, bins=50, color=color)\n",
        "    ax.axvline(threshold, color='red', linestyle='--', label='Soglia 90° percentile')\n",
        "    for patch, left in zip(patches, bins[:-1]):\n",
        "        if left >= threshold:\n",
        "            patch.set_edgecolor('red')\n",
        "            patch.set_linewidth(2)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"MSE\")\n",
        "    ax.legend()\n",
        "\n",
        "# Seconda riga: divisione per cluster\n",
        "cluster_info = [\n",
        "    (test_errors_l, labels_low, \"Test LOW - Cluster\", ['goldenrod', 'orangered']),\n",
        "    (test_errors_h, labels_high, \"Test HIGH - Cluster\", ['indigo', 'mediumorchid'])\n",
        "]\n",
        "\n",
        "for ax, (errors, labels, title_prefix, cluster_colors) in zip(axs[1], cluster_info):\n",
        "    for cluster_id, color in zip([0, 1], cluster_colors):\n",
        "        cluster_errors = errors[labels == cluster_id]\n",
        "        ax.hist(cluster_errors, bins=50, alpha=0.7, label=f\"{title_prefix} {cluster_id}\", color=color)\n",
        "    ax.axvline(threshold, color='red', linestyle='--', label='Soglia 90° percentile')\n",
        "    ax.set_xlabel(\"MSE\")\n",
        "    ax.legend()\n",
        "    ax.set_title(f\"{title_prefix} - Anomaly Score by Cluster\")\n",
        "\n",
        "plt.suptitle(\"Distribuzione degli errori di ricostruzione (MSE) e divisione per cluster\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()\n",
        "\n",
        "print()\n",
        "\n",
        "# Percentuali di ciascun cluster\n",
        "def print_cluster_stats(labels, name):\n",
        "    unique, counts = np.unique(labels, return_counts=True)\n",
        "    total = counts.sum()\n",
        "    print(f\"{name}:\")\n",
        "    for u, c in zip(unique, counts):\n",
        "        print(f\"  Cluster {u}: {c} ({c / total * 100:.2f}%)\")\n",
        "    print()\n",
        "\n",
        "print_cluster_stats(labels_low, f\"Test LOW ({frac_anom_low:.3f}% di anomalie MSE)\")\n",
        "print_cluster_stats(labels_high, f\"Test HIGH ({frac_anom_high:.3f}% di anomalie MSE)\")"
      ],
      "metadata": {
        "id": "G5O5Mo6CxiP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (TASK 4) Misura della purezza dei cluster rispetto alla classificazione fatta con l'MSE\n",
        "\n",
        "def purity_score(cluster_labels, anomaly_labels):\n",
        "    N = len(cluster_labels)\n",
        "    purity_sum = 0\n",
        "    for cl in np.unique(cluster_labels):\n",
        "        idx = (cluster_labels == cl)\n",
        "        # Quanti punti \"anomali\" e \"normali\" nel cluster\n",
        "        counts = np.bincount(anomaly_labels[idx], minlength=2)\n",
        "        purity_sum += counts.max()\n",
        "    return purity_sum / N\n",
        "\n",
        "# 1. Calcola anomaly labels\n",
        "anomaly_labels_low = (test_errors_l < threshold).astype(int)   # 1 = normale\n",
        "anomaly_labels_high = (test_errors_h < threshold).astype(int)\n",
        "\n",
        "# 2. Calcola purity per LOW e HIGH\n",
        "purity_low = purity_score(labels_low, anomaly_labels_low)\n",
        "purity_high = purity_score(labels_high, anomaly_labels_high)\n",
        "\n",
        "print(f\"Purity nei cluster:\")\n",
        "print(f\"  Test LOW : {purity_low:.3f}\")\n",
        "print(f\"  Test HIGH: {purity_high:.3f}\")\n"
      ],
      "metadata": {
        "id": "DzgiDpWDyaS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modello L -> 32D"
      ],
      "metadata": {
        "id": "kifA-LR6RzSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_L = ConvAE(params_L).to(DEVICE)\n",
        "\n",
        "# Supponendo input (1, 100, 100)\n",
        "summary(model_L, input_size=(1, 100, 100))"
      ],
      "metadata": {
        "id": "Gt01G6a1j75U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training del modello Large\n",
        "\n",
        "# Inizializza il modello\n",
        "model_L = ConvAE(params_L)\n",
        "\n",
        "early_stop_cb = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=PATIENCE,\n",
        "    mode=\"min\"\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    max_epochs=MAX_EPOCHS,\n",
        "    accelerator=\"auto\",\n",
        "    callbacks=[early_stop_cb],\n",
        "    log_every_n_steps=10\n",
        ")\n",
        "\n",
        "# Training\n",
        "trainer.fit(model_L, train_loader, val_loader)\n"
      ],
      "metadata": {
        "id": "M_aJMCX5R7sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Confronto degli errori di ricostruzione: Test LOW, Test HIGH\n",
        "\n",
        "def compute_reconstruction_errors(model, dataloader):\n",
        "    model.eval()\n",
        "    errors = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            x = batch[0].to(model.device)\n",
        "            x_hat = model(x)\n",
        "            mse = F.mse_loss(x_hat, x, reduction='none')\n",
        "            mse_per_image = mse.view(mse.size(0), -1).mean(dim=1)\n",
        "            errors.extend(mse_per_image.cpu().numpy())\n",
        "    return np.array(errors)\n",
        "\n",
        "# Calcolo errori\n",
        "train_errors = compute_reconstruction_errors(model_L, train_loader)\n",
        "val_errors = compute_reconstruction_errors(model_L, val_loader)\n",
        "test_errors_l = compute_reconstruction_errors(model_L, test_loader_l)\n",
        "test_errors_h = compute_reconstruction_errors(model_L, test_loader_h)\n",
        "\n",
        "print(f\"Train MSE: {np.mean(train_errors):.7f} | Val MSE: {np.mean(val_errors):.7f}\")\n",
        "print(f\"Test LOW MSE: {np.mean(test_errors_l):.7f} | Test HIGH MSE: {np.mean(test_errors_h):.7f}\")"
      ],
      "metadata": {
        "id": "nqlIsQzF0ZlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (TASK 1) Plot 3D (Riduzione da 32D) delle rappresentazioni latenti di: Train+Val, Test LOW, Test HIGH\n",
        "\n",
        "# Estrazione\n",
        "latents_train = extract_latents(train_loader, model_L)\n",
        "latents_val = extract_latents(val_loader, model_L)\n",
        "latents_low = extract_latents(test_loader_l, model_L)\n",
        "latents_high = extract_latents(test_loader_h, model_L)\n",
        "\n",
        "# Standardizzazione\n",
        "scaler = StandardScaler()\n",
        "latents_train_std = scaler.fit_transform(latents_train)\n",
        "latents_val_std = scaler.transform(latents_val)\n",
        "latents_low_std = scaler.transform(latents_low)\n",
        "latents_high_std = scaler.transform(latents_high)\n",
        "\n",
        "# PCA → 3D\n",
        "pca = PCA(n_components=3, random_state=SEED)\n",
        "latents_train_pca = pca.fit_transform(latents_train)\n",
        "latents_val_pca = pca.transform(latents_val)\n",
        "latents_low_pca = pca.transform(latents_low)\n",
        "latents_high_pca = pca.transform(latents_high)\n",
        "\n",
        "# Subplots Plotly\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=3,\n",
        "    specs=[[{'type': 'scatter3d'}]*3],\n",
        "    subplot_titles=[\"Train\", \"Test LOW\", \"Test HIGH\"]\n",
        ")\n",
        "\n",
        "# Tracce\n",
        "fig.add_trace(plot_latents_plotly(latents_train_pca, \"Train\", \"deepskyblue\"), row=1, col=1)\n",
        "fig.add_trace(plot_latents_plotly(latents_val_pca, \"Val\", \"steelblue\"), row=1, col=1)\n",
        "fig.add_trace(plot_latents_plotly(latents_low_pca, \"Test LOW\", \"orange\"), row=1, col=2)\n",
        "fig.add_trace(plot_latents_plotly(latents_high_pca, \"Test HIGH\", \"purple\"), row=1, col=3)\n",
        "\n",
        "# Layout\n",
        "fig.update_layout(\n",
        "    height=600, width=1800,\n",
        "    title_text=\"Spazio Latente (PCA) - Train+Val vs Test\",\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "nZSFiMJv2uyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (TASK 2) Stima delle anomalie nei Test LOW e Test HIGH (tramite errore MSE)\n",
        "\n",
        "threshold = np.percentile(val_errors, 90)\n",
        "print(f\"Soglia impostata (90° percentile val set): {threshold:.6f}\")\n",
        "\n",
        "# Stima della frazione anomala\n",
        "frac_anom_low = np.mean(test_errors_l > threshold)\n",
        "frac_anom_high = np.mean(test_errors_h > threshold)\n",
        "\n",
        "# Plot\n",
        "fig, axs = plt.subplots(1, 3, figsize=(20, 5), sharey=True, sharex=True)\n",
        "datasets = [val_errors, test_errors_l, test_errors_h]\n",
        "titles = [\"Validation\", \"Test LOW\", \"Test HIGH\"]\n",
        "colors = ['steelblue', 'orange', 'purple']\n",
        "\n",
        "for ax, data, title, color in zip(axs, datasets, titles, colors):\n",
        "    n, bins, patches = ax.hist(data, bins=100, color=color)\n",
        "    ax.axvline(threshold, color='red', linestyle='--', label='Soglia 90° percentile')\n",
        "    for patch, left in zip(patches, bins[:-1]):\n",
        "        if left >= threshold:\n",
        "            patch.set_edgecolor('red')\n",
        "            patch.set_linewidth(2)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"MSE\")\n",
        "    ax.legend()\n",
        "\n",
        "plt.suptitle(\"Distribuzione degli errori di ricostruzione (MSE) per immagine\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFrazione stimata di anomalie:\")\n",
        "print(f\"Test LOW : {frac_anom_low:.3f}\")\n",
        "print(f\"Test HIGH: {frac_anom_high:.3f}\")\n",
        "print(f\"Verifica relazione: f_high > f_low → {frac_anom_high > frac_anom_low}\")\n"
      ],
      "metadata": {
        "id": "4R8kG0la5ECb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (TASK 3) Clustering delle rappresentazioni latenti con GMM\n",
        "\n",
        "\"\"\"# Funzione per rietichettare i cluster in base all'errore medio\n",
        "def reorder_gmm_labels(errors, labels):\n",
        "    cluster_means = [errors[labels == i].mean() for i in np.unique(labels)]\n",
        "    new_order = np.argsort(cluster_means)\n",
        "    label_map = {old: new for new, old in enumerate(new_order)}\n",
        "    return np.vectorize(label_map.get)(labels)\"\"\"\n",
        "\n",
        "# GMM clustering\n",
        "# Concatenazione dei dati\n",
        "latents_combined = np.concatenate([latents_low_std, latents_high_std], axis=0)\n",
        "\n",
        "# Fit del GMM su entrambi\n",
        "gmm = GaussianMixture(n_components=2, random_state=SEED)\n",
        "gmm.fit(latents_combined)\n",
        "\n",
        "# Predizione separata mantenendo coerenza dei cluster\n",
        "labels_low = gmm.predict(latents_low_std)\n",
        "labels_high = gmm.predict(latents_high_std)\n",
        "\n",
        "\n",
        "\"\"\"# Rietichettatura coerente\n",
        "labels_low = reorder_gmm_labels(test_errors_l, labels_low)\n",
        "labels_high = reorder_gmm_labels(test_errors_h, labels_high)\"\"\"\n",
        "\n",
        "# Plot interattivo\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=2,\n",
        "    specs=[[{'type': 'scatter3d'}, {'type': 'scatter3d'}]],\n",
        "    subplot_titles=[\"GMM Clustering - Test LOW\", \"GMM Clustering - Test HIGH\"]\n",
        ")\n",
        "\n",
        "fig.add_trace(go.Scatter3d(\n",
        "    x=latents_low_pca[:, 0], y=latents_low_pca[:, 1], z=latents_low_pca[:, 2],\n",
        "    mode='markers',\n",
        "    marker=dict(size=2, color=labels_low, colorscale='Viridis', opacity=0.6),\n",
        "    name=\"LOW\"\n",
        "), row=1, col=1)\n",
        "\n",
        "fig.add_trace(go.Scatter3d(\n",
        "    x=latents_high_pca[:, 0], y=latents_high_pca[:, 1], z=latents_high_pca[:, 2],\n",
        "    mode='markers',\n",
        "    marker=dict(size=2, color=labels_high, colorscale='Viridis', opacity=0.6),\n",
        "    name=\"HIGH\"\n",
        "), row=1, col=2)\n",
        "\n",
        "fig.update_layout(\n",
        "    height=600, width=1000,\n",
        "    title_text=\"GMM Clustering nello Spazio Latente\",\n",
        "    showlegend=False\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "aWr0avae5XZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Errore di Ricostruzione VS Clustering dei Test Set\n",
        "\n",
        "fig, axs = plt.subplots(2, 2, figsize=(12, 8), sharex=True, sharey=True)\n",
        "\n",
        "# Prima riga: errori complessivi\n",
        "datasets = [test_errors_l, test_errors_h]\n",
        "titles = [\"Test LOW - Anomaly Score\", \"Test HIGH - Anomaly Score\"]\n",
        "colors = ['orange', 'purple']\n",
        "\n",
        "for ax, data, title, color in zip(axs[0], datasets, titles, colors):\n",
        "    n, bins, patches = ax.hist(data, bins=50, color=color)\n",
        "    ax.axvline(threshold, color='red', linestyle='--', label='Soglia 90° percentile')\n",
        "    for patch, left in zip(patches, bins[:-1]):\n",
        "        if left >= threshold:\n",
        "            patch.set_edgecolor('red')\n",
        "            patch.set_linewidth(2)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"MSE\")\n",
        "    ax.legend()\n",
        "\n",
        "# Seconda riga: divisione per cluster\n",
        "cluster_info = [\n",
        "    (test_errors_l, labels_low, \"Test LOW - Cluster\", ['goldenrod', 'orangered']),\n",
        "    (test_errors_h, labels_high, \"Test HIGH - Cluster\", ['indigo', 'mediumorchid'])\n",
        "]\n",
        "\n",
        "for ax, (errors, labels, title_prefix, cluster_colors) in zip(axs[1], cluster_info):\n",
        "    for cluster_id, color in zip([0, 1], cluster_colors):\n",
        "        cluster_errors = errors[labels == cluster_id]\n",
        "        ax.hist(cluster_errors, bins=50, alpha=0.7, label=f\"{title_prefix} {cluster_id}\", color=color)\n",
        "    ax.axvline(threshold, color='red', linestyle='--', label='Soglia 90° percentile')\n",
        "    ax.set_xlabel(\"MSE\")\n",
        "    ax.legend()\n",
        "    ax.set_title(f\"{title_prefix} - Anomaly Score by Cluster\")\n",
        "\n",
        "plt.suptitle(\"Distribuzione degli errori di ricostruzione (MSE) e divisione per cluster\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()\n",
        "\n",
        "print()\n",
        "\n",
        "print_cluster_stats(labels_low, f\"Test LOW ({frac_anom_low:.3f}% di anomalie MSE)\")\n",
        "print_cluster_stats(labels_high, f\"Test HIGH ({frac_anom_high:.3f}% di anomalie MSE)\")"
      ],
      "metadata": {
        "id": "hoNMYn7Q58Be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (TASK 4) Misura della purezza dei cluster rispetto alla classificazione fatta con l'MSE\n",
        "\n",
        "# 1. Calcola anomaly labels\n",
        "anomaly_labels_low = (test_errors_l > threshold).astype(int)   # 1 = anomalia\n",
        "anomaly_labels_high = (test_errors_h > threshold).astype(int)\n",
        "\n",
        "# 2. Calcola purity per LOW e HIGH\n",
        "purity_low = purity_score(labels_low, anomaly_labels_low)\n",
        "purity_high = purity_score(labels_high, anomaly_labels_high)\n",
        "\n",
        "print(f\"Purity nei cluster:\")\n",
        "print(f\"  Test LOW : {purity_low:.3f}\")\n",
        "print(f\"  Test HIGH: {purity_high:.3f}\")\n"
      ],
      "metadata": {
        "id": "J_u1zwP86Oyg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}